{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, BatchNormalization, Activation, Input, Add, Concatenate\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n",
    "    Conv1D, MaxPool1D, Flatten, Concatenate, Add, MaxPooling1D,LSTM\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df[\"review_id\"], df[\"text\"], df[\"stars\"]\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)[\"stars\"]\n",
    "\n",
    "def write_predictions(file_name, pred):\n",
    "    df = pd.DataFrame(zip(range(len(pred)), pred))\n",
    "    df.columns = [\"review_id\", \"stars\"]\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type|: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "    \n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
    "    :param max_size: the max size of feature dict, type: int\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
    "    else:\n",
    "        valid_feats = list()\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]        \n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float32)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector\n",
    "\n",
    "def get_count_vector(feats, feats_dict): \n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float32)\n",
    "    for f in feats:\n",
    "            # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1) \n",
    "        if f_idx != -1:\n",
    "                # add the corresponding element by 1\n",
    "                vector[f_idx] += 1 \n",
    "    return vector\n",
    "\n",
    "def get_idf_dict(feats_list): \n",
    "    \"\"\"\n",
    "    :param feats_list: a list of lists of features, type: list(list) return an idf vector,\n",
    "    \"\"\"\n",
    "    N = len(feats_list)\n",
    "    df_dict = Counter()\n",
    "    for feats in feats_list:\n",
    "        df_dict.update(set(feats))\n",
    "        # IDF: log(1 + N/n)\n",
    "        idf_dict = {f: math.log2(1+N/cnt) for f, cnt in df_dict.items()}\n",
    "    return idf_dict\n",
    "\n",
    "def get_tfidf_vector(feats, feats_dict, idf_dict): \n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict :param idf_dict: a dict from features to idf, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float32)\n",
    "    # TF: 1 + log(f)\n",
    "    tf_dict = {f: 1+math.log2(cnt) for f, cnt in Counter(feats).items()} \n",
    "    for f in tf_dict:\n",
    "            # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1) \n",
    "        if f_idx != -1:\n",
    "            tf = tf_dict[f]\n",
    "            idf = idf_dict[f]\n",
    "            # set the corresponding element as tf*idf \n",
    "            vector[f_idx] = tf*idf\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 50501\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/valid.csv\"\n",
    "#ans_file = \"data/ans.csv\"\n",
    "pred_file = \"data/pred.csv\"\n",
    "min_freq = 3\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(test_file)\n",
    "\n",
    "# extract features\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_stemmed = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "test_stemmed = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
    "\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "test_2_gram = [n_gram(tokens, 2) for tokens in test_stemmed]\n",
    "test_3_gram = [n_gram(tokens, 3) for tokens in test_stemmed]\n",
    "\n",
    "# build the feature list\n",
    "train_feats = list()\n",
    "for i in range(len(train_ids)):\n",
    "    train_feats.append(\n",
    "        train_stemmed[i] + train_2_gram[i])\n",
    "test_feats = list()\n",
    "for i in range(len(test_ids)):\n",
    "    test_feats.append(\n",
    "        test_stemmed[i] + test_2_gram[i])\n",
    "\n",
    "# build a mapping from features to indices\n",
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_feats),\n",
    "    min_freq=5)\n",
    "\n",
    "train_feats_matrix = np.vstack(\n",
    "    [get_onehot_vector(f, feats_dict) for f in train_feats])\n",
    "test_feats_matrix = np.vstack(\n",
    "    [get_onehot_vector(f, feats_dict) for f in test_feats])\n",
    "\n",
    "# convert labels to label_matrix\n",
    "num_classes = max(train_labels)\n",
    "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
    "train_label_matrix = keras.utils.to_categorical(train_labels-1, num_classes=num_classes)\n",
    "test_label_matrix = keras.utils.to_categorical(test_labels-1, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrixx = np.zeros((len(feats_dict), 100), dtype=np.float32)\n",
    "with open('models/glove.6B.100d.txt',  encoding='utf8') as f:\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        word, vec = line.split(\" \", 1)\n",
    "        word_idx = feats_dict.get(word, -1)\n",
    "        if word_idx != -1:\n",
    "            embedding_matrixx[word_idx] = np.array(vec.split(), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 3970 word dimension: 100\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(feats_dict), 100), dtype=np.float32)\n",
    "with open(\"models/word2vec.vec\", \"r\") as f:\n",
    "    n_words, n_dim = f.readline().split()\n",
    "    n_words, n_dim = int(n_words), int(n_dim) \n",
    "    print(\"number of words:\", n_words, \"word dimension:\", n_dim)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        word, vec = line.split(\" \", 1)\n",
    "        word_idx = feats_dict.get(word, -1)\n",
    "        if word_idx != -1:\n",
    "            embedding_matrix[word_idx] = np.array(vec.split(), dtype=np.float32)\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Res_Net(input_size, vocab_size, embedding_size, \n",
    "                 output_size, num_layers, hidden_size,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              layer_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\",embedding_matrix = embedding_matrixx):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_layers: the number of layers, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param layer_norm: whether to enable layer normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a multi-layer network with residual connections,\n",
    "    # activation\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # layer normalization: https://github.com/CyberZHG/keras-layer-normalization\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_size,))\n",
    "\n",
    "    y = Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_size,\n",
    "                        input_length=input_size,\n",
    "                        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                        trainable=True)(x)\n",
    "    if num_layers == 1:\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  input_dim=input_size,\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\",\n",
    "                  kernel_regularizer=keras.regularizers.l2(l2_reg))(y)\n",
    "    else:\n",
    "        h = x\n",
    "        for i in range(num_layers-1):\n",
    "            if i == 0:\n",
    "                # fitst layer: input -> hidden\n",
    "                new_h = Dense(hidden_size,\n",
    "                          input_dim=input_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            else:\n",
    "                new_h = Dense(hidden_size,\n",
    "                          input_dim=hidden_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            # add layer_norm\n",
    "            if layer_norm:\n",
    "                new_h = LayerNormalization()(new_h)\n",
    "            # add batch_norm\n",
    "            if batch_norm:\n",
    "                new_h = BatchNormalization()(new_h)\n",
    "            # residual connection\n",
    "            if i == 0:\n",
    "                h = new_h\n",
    "            else:\n",
    "                h = Add()([h, new_h])\n",
    "            # add activation\n",
    "            h = Activation(activation)(h)\n",
    "            # add dropout here (set seed as 0 in order to reproduce)\n",
    "            if dropout_rate > 0.0:\n",
    "                h = Dropout(dropout_rate, seed=0)(h)\n",
    "        # last layer: hidden -> class\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  input_dim=hidden_size,\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 102us/step\n",
      "2000/2000 [==============================] - 0s 101us/step\n",
      "training loss: 1.2378405725955963 training accuracy 0.7918000221252441\n",
      "test loss: 1.4425039887428284 test accuracy 0.6629999876022339\n"
     ]
    }
   ],
   "source": [
    "model = build_Res_Net(input_size=len(feats_dict), vocab_size = len(feats_dict),embedding_size=100,output_size=num_classes,\n",
    "                  num_layers=3, hidden_size=100, activation=\"relu\",\n",
    "                  l2_reg=0.006, dropout_rate=0.3)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "res_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=20, batch_size=100, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b015424011cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/valid.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stars\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/pred.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stars\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stars_x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stars_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     45\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# to avoid incompat dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# If argument passed to validate,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    978\u001b[0m                       (inferred_right in string_types and\n\u001b[1;32m    979\u001b[0m                        inferred_left not in string_types)):\n\u001b[0;32m--> 980\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0;31m# datetimelikes must match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# save predictions\n",
    "pred_file = \"data/pred.csv\"\n",
    "#test_pred = model.predict_classes(test_feats_matrix)\n",
    "test_pred = model.predict(test_feats_matrix)\n",
    "test_pred = np.argmax(test_pred,axis=1)\n",
    "write_predictions(pred_file, test_pred)\n",
    "ans = pd.read_csv(\"data/valid.csv\", usecols=[\"review_id\", \"stars\"])\n",
    "pred = pd.read_csv(\"data/pred.csv\", usecols=[\"review_id\", \"stars\"])\n",
    "df = pd.merge(ans, pred, how=\"left\", on=[\"review_id\"])\n",
    "df.fillna(0, inplace=True)\n",
    "acc = accuracy_score(df[\"stars_x\"], df[\"stars_y\"])\n",
    "p, r, f1, _ = precision_recall_fscore_support(df[\"stars_x\"], df[\"stars_y\"], average=\"macro\")\n",
    "print(\"accuracy:\", acc, \"\\tprecision:\", p, \"\\trecall:\", r, \"\\tf1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
