{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.8.32-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 14 kB/s eta 0:00:017\n",
      "\u001b[?25hCollecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting gql==0.2.0\n",
      "  Downloading gql-0.2.0.tar.gz (18 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "Collecting watchdog>=0.8.3\n",
      "  Downloading watchdog-0.10.2.tar.gz (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 20 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 40 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from wandb) (3.13)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from wandb) (2.19.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from wandb) (5.4.7)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting Click>=7.0\n",
      "  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 18 kB/s eta 0:00:015\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.1-py3-none-any.whl (450 kB)\n",
      "\u001b[K     |████████████████████████████████| 450 kB 40 kB/s eta 0:00:014\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from wandb) (1.11.0)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "  Downloading sentry_sdk-0.14.3-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 30 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from wandb) (2.7.3)\n",
      "Collecting graphql-core<2,>=0.5.0\n",
      "  Downloading graphql-core-1.1.tar.gz (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 38 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting pathtools>=0.1.1\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->wandb) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->wandb) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/xuyingzi/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->wandb) (2.7)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.4-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 22 kB/s eta 0:00:015\n",
      "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.2-py2.py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: nvidia-ml-py3, gql, watchdog, subprocess32, graphql-core, promise, pathtools\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19198 sha256=eec7ff8f73b50bcfb88edb2db58a2a4f05f7260375ced5efe89818e545bdaa3a\n",
      "  Stored in directory: /Users/xuyingzi/Library/Caches/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n",
      "  Building wheel for gql (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-py3-none-any.whl size=6821 sha256=19cf9da2ef28b7ff8aca5fe11a1a3c0471d5209ea6d37289bad10a3af498a23b\n",
      "  Stored in directory: /Users/xuyingzi/Library/Caches/pip/wheels/b6/9a/56/5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.2-cp37-cp37m-macosx_10_7_x86_64.whl size=72187 sha256=b5dba3aea6285d3595ecaf2e8d0a11cc6c0b70f91b855f778a60dc85388679e1\n",
      "  Stored in directory: /Users/xuyingzi/Library/Caches/pip/wheels/36/93/24/29e375ee74e0f178889e6906cb73e693e9f06a5f589dcee6b9\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=2038 sha256=08df7f69d4a94de2417194d723fe82a882e4daf01544cfcbaab558093bd30f4e\n",
      "  Stored in directory: /Users/xuyingzi/Library/Caches/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for graphql-core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-py3-none-any.whl size=104657 sha256=c00be7cec851786e8a537113256ae351b6acb3679545b8b65c2dc2df24709081\n",
      "  Stored in directory: /Users/xuyingzi/Library/Caches/pip/wheels/6b/fd/8c/a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=20683 sha256=04a5cc4258757b55b89880bab8b5d5df7861b2b3c50bc133ace70d566fcbfa7f\n",
      "  Stored in directory: /Users/xuyingzi/Library/Caches/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=7678 sha256=6ab6958a6992c059c5dbd03d3bfe6cb46fe1d46a0afcc0d1d345520f33b788e5\n",
      "  Stored in directory: /Users/xuyingzi/Library/Caches/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built nvidia-ml-py3 gql watchdog subprocess32 graphql-core promise pathtools\n",
      "Installing collected packages: nvidia-ml-py3, promise, graphql-core, gql, shortuuid, pathtools, watchdog, subprocess32, configparser, Click, docker-pycreds, smmap, gitdb, GitPython, sentry-sdk, wandb\n",
      "  Attempting uninstall: Click\n",
      "    Found existing installation: click 6.7\n",
      "    Uninstalling click-6.7:\n",
      "      Successfully uninstalled click-6.7\n",
      "Successfully installed Click-7.1.1 GitPython-3.1.1 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.4 gql-0.2.0 graphql-core-1.1 nvidia-ml-py3-7.352.0 pathtools-0.1.2 promise-2.3 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.2 subprocess32-3.5.4 wandb-0.8.32 watchdog-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Activation, Input, Add, Concatenate\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides a lot of packages to load files in different formats. We provide a simple data loader to help you load .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df[\"review_id\"], df[\"text\"], df[\"stars\"]\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)[\"stars\"]\n",
    "\n",
    "def write_predictions(file_name, pred):\n",
    "    df = pd.DataFrame(zip(range(len(pred)), pred))\n",
    "    df.columns = [\"review_id\", \"stars\"]\n",
    "    df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **feature extractor** is one of the most important parts in a pipeline.\n",
    "In this tutorial, we introduce four different functions to extract features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "    \n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n",
    "    :param max_size: the max size of feature dict, type: int\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
    "    else:\n",
    "        valid_feats = list()\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]        \n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we introduce a 1-layer perceptron to classify reviews. This perceptron includes 1 dense layer with the softmax activation.\n",
    "Keras is the easiest deep learning framework so that we choose it to build this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_size, output_size,\n",
    "                     l2_reg=0.0,\n",
    "                     loss=\"categorical_crossentropy\",\n",
    "                     optimizer=\"SGD\",\n",
    "                     learning_rate=0.1,\n",
    "                     metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a 1-layer perceptron,\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # the projection layer\n",
    "    model.add(Dense(output_size,\n",
    "                    activation=\"softmax\",\n",
    "                    input_dim=input_size,\n",
    "                    kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                    bias_initializer=\"zeros\",\n",
    "                    kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Data Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data loader, feature extractor, and the classifier. We can connect them to finish this pipeline of classification. We follow the setting in Tutorial 2 and only select the features whose frequencies are no less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 71646\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/valid.csv\"\n",
    "#ans_file = \"data/ans.csv\"\n",
    "#pred_file = \"data/pred.csv\"\n",
    "min_freq = 3\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(test_file)\n",
    "\n",
    "# extract features\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_stemmed = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "test_stemmed = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
    "\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "test_2_gram = [n_gram(tokens, 2) for tokens in test_stemmed]\n",
    "test_3_gram = [n_gram(tokens, 3) for tokens in test_stemmed]\n",
    "\n",
    "# build the feature list\n",
    "train_feats = list()\n",
    "for i in range(len(train_ids)):\n",
    "    train_feats.append(\n",
    "        train_stemmed[i] + train_2_gram[i] + train_3_gram[i])\n",
    "test_feats = list()\n",
    "for i in range(len(test_ids)):\n",
    "    test_feats.append(\n",
    "        test_stemmed[i] + test_2_gram[i] + test_3_gram[i])\n",
    "\n",
    "# build a mapping from features to indices\n",
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_feats),\n",
    "    min_freq=5)\n",
    "\n",
    "train_feats_matrix = np.vstack(\n",
    "    [get_onehot_vector(f, feats_dict) for f in train_feats])\n",
    "test_feats_matrix = np.vstack(\n",
    "    [get_onehot_vector(f, feats_dict) for f in test_feats])\n",
    "\n",
    "# convert labels to label_matrix\n",
    "num_classes = max(train_labels)\n",
    "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
    "train_label_matrix = keras.utils.to_categorical(train_labels-1, num_classes=num_classes)\n",
    "test_label_matrix = keras.utils.to_categorical(test_labels-1, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vital change in multi-layer perceptron is the number of layers. We stack more layers so that extracted features can be further enhanced in hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP(input_size, output_size, num_layers, hidden_size,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.01,\n",
    "              batch_norm=True,\n",
    "              layer_norm=False,\n",
    "              l2_reg=0.001,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_layers: the number of layers, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param layer_norm: whether to enable layer normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a multi-layer perceptron,\n",
    "    # activation\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # layer normalization: https://github.com/CyberZHG/keras-layer-normalization\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        model.add(Dense(output_size,\n",
    "                        activation=\"softmax\",\n",
    "                        input_dim=input_size,\n",
    "                        kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                        bias_initializer=\"zeros\",\n",
    "                        kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "    else:\n",
    "        for i in range(num_layers-1):\n",
    "            if i == 0:\n",
    "                # fitst layer: input -> hidden\n",
    "                model.add(Dense(hidden_size,\n",
    "                                input_dim=input_size,\n",
    "                                kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                                bias_initializer=\"zeros\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "            else:\n",
    "                # hidden layers: hidden -> hidden\n",
    "                model.add(Dense(hidden_size,\n",
    "                                input_dim=hidden_size,\n",
    "                                kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                                bias_initializer=\"zeros\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "            # add layer_norm\n",
    "            if layer_norm:\n",
    "                model.add(LayerNormalization())\n",
    "            # add batch_norm\n",
    "            if batch_norm:\n",
    "                model.add(BatchNormalization())\n",
    "            # add activation\n",
    "            model.add(Activation(activation))\n",
    "            # add dropout here (set seed as 0 in order to reproduce)\n",
    "            if dropout_rate > 0.0:\n",
    "                model.add(Dropout(dropout_rate, seed=0))\n",
    "        # last layer: hidden -> class\n",
    "        model.add(Dense(output_size,\n",
    "                        activation=\"softmax\",\n",
    "                        input_dim=hidden_size,\n",
    "                        kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                        bias_initializer=\"zeros\"))\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we build a single layer perceptron, which can achieve test 57.75% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 21s 1ms/step\n",
      "2000/2000 [==============================] - 2s 1ms/step\n",
      "training loss: 0.6165140730857849 training accuracy 0.8051000237464905\n",
      "test loss: 0.8969401912689209 test accuracy 0.6449999809265137\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model = build_classifier(input_size=len(feats_dict), output_size=num_classes,\n",
    "                         l2_reg=0.0001)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "slp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=64, verbose=0,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=64)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=64)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a 3-layer MLP whose hidden size is 100. We choose the **ReLU** as activations in hidden layers, which is used more widely. And another popular activation is **Tanh**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](ReLU.png) ![Tanh](Tanh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 26s 1ms/step\n",
      "2000/2000 [==============================] - 2s 1ms/step\n",
      "training loss: 1.7992390644550325 training accuracy 0.9331499934196472\n",
      "test loss: 3.0175549149513246 test accuracy 0.5995000004768372\n"
     ]
    }
   ],
   "source": [
    "model = build_MLP(input_size=len(feats_dict), output_size=num_classes,\n",
    "                  num_layers=3, hidden_size=100, activation=\"relu\",\n",
    "                  l2_reg=0.005)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "mlp_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=0,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(slp_history.history[\"loss\"], label=\"SLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(slp_history.history[\"val_loss\"], label=\"SLP-validation\", color=\"blue\")\n",
    "plt.plot(mlp_history.history[\"loss\"], label=\"MLP-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(mlp_history.history[\"val_loss\"], label=\"MLP-validation\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(slp_history.history[\"accuracy\"], label=\"SLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(slp_history.history[\"val_accuracy\"], label=\"SLP-validation\", color=\"blue\")\n",
    "plt.plot(mlp_history.history[\"accuracy\"], label=\"MLP-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(mlp_history.history[\"val_accuracy\"], label=\"MLP-validation\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this MLP is not as good as SLP. When we check curves, we find MLP can only achieve a better training accuracy. Hence, we also need to solve the overfitting (to improve the test performance) and underfitting (to decrease the training loss) problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies to Reduce Overfitting Con't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the dropout rate as 0.2 for MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 28s 1ms/step\n",
      "2000/2000 [==============================] - 3s 1ms/step\n",
      "training loss: 1.9498669070005417 training accuracy 0.9010499715805054\n",
      "test loss: 2.5224180102348326 test accuracy 0.6294999718666077\n"
     ]
    }
   ],
   "source": [
    "model = build_MLP(input_size=len(feats_dict), output_size=num_classes,\n",
    "                  num_layers=3, hidden_size=100, activation=\"relu\",\n",
    "                  l2_reg=0.005, dropout_rate=0.3)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "drop_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=0,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(mlp_history.history[\"loss\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(mlp_history.history[\"val_loss\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(drop_history.history[\"loss\"], label=\"MLP-training (dropout)\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_loss\"], label=\"MLP-validation (dropout)\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(mlp_history.history[\"accuracy\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(mlp_history.history[\"val_accuracy\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(drop_history.history[\"accuracy\"], label=\"MLP-training (dropout)\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_accuracy\"], label=\"MLP-validation (dropout)\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout improves the model performance from 54.25% to 58.25%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies to Reduce Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly introduce the most common two methods to help model converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When models know nothing, they must perform worse in face of the first batch examples. And then each model will update its parameters. But the learning rate controls the weight updating. Let's consider two models. Elements of inputs of the first model are always 0 or 1, and inputs of the other model are always the scaled inputs of the first, e.g., 0 or 100. If we provide infinite time and same settings to train the two models with same data except the scale, the two models must be equivalent. But the first model is very likely to converge first because of the smaller scale. Several normalization methods are proposed to address this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch normalization is designed to find feature distributions from a batch of data, while the layer normalization is used to find feature distributions from each example. For example, if 0-th feature is always 0 or 100, then the batch normalization can find it; if the average length of sentences is about 20, then the layer normalization can find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try batch normalization, layer normalization, and both. In order to reduce other effects, we disable the regularization and the dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = list()\n",
    "for batch_norm in [False, True]:\n",
    "    for layer_norm in [False, True]:\n",
    "        model = build_MLP(input_size=len(feats_dict), output_size=num_classes,\n",
    "                          num_layers=3, hidden_size=100, activation=\"relu\",\n",
    "                          batch_norm=batch_norm, layer_norm=layer_norm)\n",
    "        checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "            monitor=\"val_accuracy\",\n",
    "            verbose=0,\n",
    "            save_best_only=True)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        tf.random.set_random_seed(0)\n",
    "        history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                            validation_split=0.1,\n",
    "                            epochs=20, batch_size=100, verbose=0,\n",
    "                            callbacks=[checkpointer])\n",
    "        model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"),\n",
    "                                        custom_objects={\"LayerNormalization\": LayerNormalization})\n",
    "\n",
    "        train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                                     batch_size=100)\n",
    "        test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                                    batch_size=100)\n",
    "        \n",
    "        histories.append((batch_norm, layer_norm, history))\n",
    "        print(\"batch normalization:\", batch_norm, \"layer normalization:\", layer_norm)\n",
    "        print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "        print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"orange\", \"gray\", \"red\", \"blue\"]\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "for i, (batch_norm, layer_norm, history) in enumerate(histories):\n",
    "    if batch_norm and layer_norm:\n",
    "        name = \" (bn & ln)\"\n",
    "    elif batch_norm and not layer_norm:\n",
    "        name = \" (bn)\"\n",
    "    elif not batch_norm and layer_norm:\n",
    "        name = \" (ln)\"\n",
    "    else:\n",
    "        name = \"\"\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"MLP-training\" + name, color=colors[i], linestyle=\"dashed\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"MLP-validation\" + name, color=colors[i])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"MLP-training\" + name, color=colors[i], linestyle=\"dashed\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"MLP-validation\" + name, color=colors[i])\n",
    "plt.subplot(1,2,1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all normalization can improve the training performance significantly and test performance slightly. Once we choose these normalization methods, we still need to use strategies to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Res_Net(input_size, output_size, num_layers, hidden_size,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              layer_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_layers: the number of layers, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param layer_norm: whether to enable layer normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a multi-layer network with residual connections,\n",
    "    # activation\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # layer normalization: https://github.com/CyberZHG/keras-layer-normalization\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_size,))\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  input_dim=input_size,\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\",\n",
    "                  kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "    else:\n",
    "        h = x\n",
    "        for i in range(num_layers-1):\n",
    "            if i == 0:\n",
    "                # fitst layer: input -> hidden\n",
    "                new_h = Dense(hidden_size,\n",
    "                          input_dim=input_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            else:\n",
    "                new_h = Dense(hidden_size,\n",
    "                          input_dim=hidden_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            # add layer_norm\n",
    "            if layer_norm:\n",
    "                new_h = LayerNormalization()(new_h)\n",
    "            # add batch_norm\n",
    "            if batch_norm:\n",
    "                new_h = BatchNormalization()(new_h)\n",
    "            # residual connection\n",
    "            if i == 0:\n",
    "                h = new_h\n",
    "            else:\n",
    "                h = Add()([h, new_h])\n",
    "            # add activation\n",
    "            h = Activation(activation)(h)\n",
    "            # add dropout here (set seed as 0 in order to reproduce)\n",
    "            if dropout_rate > 0.0:\n",
    "                h = Dropout(dropout_rate, seed=0)(h)\n",
    "        # last layer: hidden -> class\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  input_dim=hidden_size,\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_Res_Net(input_size=len(feats_dict), output_size=num_classes,\n",
    "                  num_layers=3, hidden_size=100, activation=\"relu\",\n",
    "                  l2_reg=0.005, dropout_rate=0.1)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "res_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=20, batch_size=100, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(drop_history.history[\"loss\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_loss\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(res_history.history[\"loss\"], label=\"Res-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(res_history.history[\"val_loss\"], label=\"Res-validation\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(drop_history.history[\"accuracy\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_accuracy\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(res_history.history[\"accuracy\"], label=\"Res-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(res_history.history[\"val_accuracy\"], label=\"Res-validation\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual connections can also decrease the training loss, especially in the beginning of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Cat_Net(input_size, output_size, num_layers, hidden_size,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              layer_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_layers: the number of layers, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param layer_norm: whether to enable layer normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a multi-layer networks with concatenations,\n",
    "    # activation\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # layer normalization: https://github.com/CyberZHG/keras-layer-normalization\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_size,))\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  input_dim=input_size,\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\",\n",
    "                  kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "    else:\n",
    "        h = x\n",
    "        for i in range(num_layers-1):\n",
    "            if i == 0:\n",
    "                # fitst layer: input -> hidden\n",
    "                new_h = Dense(hidden_size,\n",
    "                          input_dim=input_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            else:\n",
    "                new_h = Dense(hidden_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            # add layer_norm\n",
    "            if layer_norm:\n",
    "                new_h = LayerNormalization()(new_h)\n",
    "            # add batch_norm\n",
    "            if batch_norm:\n",
    "                new_h = BatchNormalization()(new_h)\n",
    "            # residual connection\n",
    "            if i == 0:\n",
    "                h = new_h\n",
    "            else:\n",
    "                h = Concatenate()([h, new_h])\n",
    "            # add activation\n",
    "            h = Activation(activation)(h)\n",
    "            # add dropout here (set seed as 0 in order to reproduce)\n",
    "            if dropout_rate > 0.0:\n",
    "                h = Dropout(dropout_rate, seed=0)(h)\n",
    "        # last layer: hidden -> class\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_Cat_Net(input_size=len(feats_dict), output_size=num_classes,\n",
    "                  num_layers=3, hidden_size=100, activation=\"relu\",\n",
    "                  l2_reg=0.005, dropout_rate=0.0)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "cat_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=20, batch_size=100, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(drop_history.history[\"loss\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_loss\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(res_history.history[\"loss\"], label=\"Res-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(res_history.history[\"val_loss\"], label=\"Res-validation\", color=\"orange\")\n",
    "plt.plot(cat_history.history[\"loss\"], label=\"Cat-training\", color=\"red\", linestyle=\"dashed\")\n",
    "plt.plot(cat_history.history[\"val_loss\"], label=\"Cat-validation\", color=\"red\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(drop_history.history[\"accuracy\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_accuracy\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(res_history.history[\"accuracy\"], label=\"Res-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(res_history.history[\"val_accuracy\"], label=\"Res-validation\", color=\"orange\")\n",
    "plt.plot(cat_history.history[\"accuracy\"], label=\"Cat-training\", color=\"red\", linestyle=\"dashed\")\n",
    "plt.plot(cat_history.history[\"val_accuracy\"], label=\"Cat-validation\", color=\"red\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concatenation has similar training curves to the model with residual connections. One drawback is the increased amount of parameters. When the network goes deeper, you need to balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Powerful Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned so many strategies to avoid overfitting and underfitting. You can use all of them or some of them to improve your own deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=20, batch_size=100, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"),\n",
    "                                custom_objects={\"LayerNormalization\": LayerNormalization})\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(drop_history.history[\"loss\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_loss\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(history.history[\"loss\"], label=\"Yours-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Yours-validation\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(drop_history.history[\"accuracy\"], label=\"MLP-training\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(drop_history.history[\"val_accuracy\"], label=\"MLP-validation\", color=\"blue\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Yours-training\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Yours-validation\", color=\"orange\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial and previous tutorials, we use the stochastic gradient descent optimizer to optimize the training loss. In fact, there is much cutting-edge research work about optimization. One of the most common is Adam, you can try it by setting the parameter *optimizer*. Of course, Keras provides many more choices for users and you can choose by your validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the batch normalization and the layer normalization to help model converge. But more robust and higher-efficiency normalization methods have been proposed, such as the instance normalization and the group normalization. If you are interested in normalization, you can read related articles and implement some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have moved from the single layer perceptron to the multi-layer perceptron. But that's still not enough. The residual connection and the concatenation can improve deeper networks. Building highway networks is also a way to increase the power of multiple layers. Google also designs a wide & deep network for memorization and generalization. These architectures are still based on these simple linear layers and activations. If you can grasp strategies in tutorial 2 and tutorial 3, I believe you can design promising networks on your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
